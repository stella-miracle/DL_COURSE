{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMEBX7VB8Nu4FdtXIhVV+dE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rOmnNFaiVbjA","executionInfo":{"status":"ok","timestamp":1742203554055,"user_tz":0,"elapsed":4479,"user":{"displayName":"uuny E","userId":"11907513080309152518"}},"outputId":"457581cd-c4ed-41e4-b700-b2f91b2a0230"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 174ms/step - accuracy: 0.4705 - loss: 0.7564\n","Epoch 2/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5115 - loss: 0.7032 \n","Epoch 3/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5690 - loss: 0.6987 \n","Epoch 4/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5453 - loss: 0.6998 \n","Epoch 5/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5092 - loss: 0.7237 \n","Epoch 6/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5092 - loss: 0.7181 \n","Epoch 7/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4655 - loss: 0.7225 \n","Epoch 8/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4591 - loss: 0.7051 \n","Epoch 9/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6120 - loss: 0.6663 \n","Epoch 10/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5719 - loss: 0.6717 \n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.5211 - loss: 0.6924\n","loss :0.691268801689148\n","accuracy :0.5299999713897705\n"]}],"source":["# instruction\n","# 1. add dropout layer after each hidden layer in the model\n","# 2. set the dropout rate to 0.5\n","# 3. recompile, train, evalutate the model\n","\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n","import numpy as np\n","\n","input_layer = Input(shape=(20,))\n","hidden_layer = Dense(64, activation= 'relu')(input_layer)\n","drop_layer = Dropout(rate=0.5)(hidden_layer)\n","\n","hidden_layer2 = Dense(32, activation= 'relu')(drop_layer)\n","drop_layer2 = Dropout(rate=0.5)(hidden_layer2)\n","\n","output_layer = Dense(1,activation=\"sigmoid\")(drop_layer2)\n","\n","model = Model(inputs=input_layer,outputs=output_layer)\n","model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n","\n","X_train = np.random.rand(200,20)\n","# 200개의 샘플 그리고 그 200개는 20개의 feature 를 가지고 있음.\n","y_train = np.random.randint(2, size=(200,1))\n","# x_train 의 값의 정답데이터 (label). 1000개의 라벨 (0이상2미만 실수이므로 0~1)\n","model.fit(X_train,y_train, epochs=10, batch_size=32)\n","\n","x_test = np.random.rand(1000,20)\n","# 모델이 학습 후 실제로 성능을 평가하는 입력 데이터.\n","# x_train 과 같은 feature 의 값을 가져야하고 처음보는 데이터로 평가해야하므로 새로운 데이터를 가져옴.\n","Y_test = np.random.randint(2, size=(1000,1))\n","# x_test 에 대한 결과값 (label)\n","\n","loss,accuracy = model.evaluate(x_test,Y_test)\n","print(f\"loss :{loss}\")\n","print(f\"accuracy :{accuracy}\")\n"]},{"cell_type":"code","source":["# instruction\n","# 1. changed the activation function of the hidden layer relu to tanh\n","# 2. recompile , train, evalute\n","# 3.\n","\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n","import numpy as np\n","\n","input_layer = Input(shape=(20,))\n","hidden_layer = Dense(64, activation= 'tanh')(input_layer)\n","drop_layer = Dropout(rate=0.5)(hidden_layer)\n","\n","hidden_layer2 = Dense(32, activation= 'tanh')(drop_layer)\n","drop_layer2 = Dropout(rate=0.5)(hidden_layer2)\n","\n","output_layer = Dense(1,activation=\"sigmoid\")(drop_layer2)\n","\n","model = Model(inputs=input_layer,outputs=output_layer)\n","model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n","\"\"\"\n","\n","X_train = np.random.rand(200,20)\n","# 200개의 샘플 그리고 그 200개는 20개의 feature 를 가지고 있음.\n","y_train = np.random.randint(2, size=(200,1))\n","# x_train 의 값의 정답데이터 (label). 1000개의 라벨 (0이상2미만 실수이므로 0~1)\n","\"\"\"\n","\n","model.fit(X_train,y_train, epochs=10, batch_size=32)\n","\n","\"\"\"\n","\n","x_test = np.random.rand(1000,20)\n","# 모델이 학습 후 실제로 성능을 평가하는 입력 데이터.\n","# x_train 과 같은 feature 의 값을 가져야하고 처음보는 데이터로 평가해야하므로 새로운 데이터를 가져옴.\n","Y_test = np.random.randint(2, size=(1000,1))\n","# x_test 에 대한 결과값 (label)\n","\"\"\"\n","\n","loss,accuracy = model.evaluate(x_test,Y_test)\n","print(f\"loss :{loss}\")\n","print(f\"accuracy :{accuracy}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lg1eM7WSWQt2","executionInfo":{"status":"ok","timestamp":1742203695385,"user_tz":0,"elapsed":7231,"user":{"displayName":"uuny E","userId":"11907513080309152518"}},"outputId":"41e72855-ce14-4a2a-8fe8-a1270bb41e28"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 238ms/step - accuracy: 0.4465 - loss: 0.8038\n","Epoch 2/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.4857 - loss: 0.7661\n","Epoch 3/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4404 - loss: 0.7940 \n","Epoch 4/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5592 - loss: 0.7332 \n","Epoch 5/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5672 - loss: 0.6950 \n","Epoch 6/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5018 - loss: 0.7494 \n","Epoch 7/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5476 - loss: 0.7067 \n","Epoch 8/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4792 - loss: 0.7331 \n","Epoch 9/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5284 - loss: 0.7633 \n","Epoch 10/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5452 - loss: 0.7231 \n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4770 - loss: 0.7037\n","loss :0.7014495730400085\n","accuracy :0.4959999918937683\n"]}]},{"cell_type":"code","source":["# instruction\n","# 1. add Batchnormalization\n","\n","\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n","import numpy as np\n","\n","input_layer = Input(shape=(20,))\n","hidden_layer = Dense(64, activation= 'relu')(input_layer)\n","# drop_layer = Dropout(rate=0.5)(hidden_layer)\n","batch_layer = BatchNormalization()(hidden_layer)\n","\n","hidden_layer2 = Dense(32, activation= 'relu')(hidden_layer)\n","#drop_layer2 = Dropout(rate=0.5)(hidden_layer2)\n","batch_layer2 = BatchNormalization()(hidden_layer2)\n","\n","output_layer = Dense(1,activation=\"sigmoid\")(hidden_layer2)\n","\n","model = Model(inputs=input_layer,outputs=output_layer)\n","model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n","\n","X_train = np.random.rand(200,20)\n","# 200개의 샘플 그리고 그 200개는 20개의 feature 를 가지고 있음.\n","y_train = np.random.randint(2, size=(200,1))\n","# x_train 의 값의 정답데이터 (label). 1000개의 라벨 (0이상2미만 실수이므로 0~1)\n","model.fit(X_train,y_train, epochs=10, batch_size=32)\n","\n","x_test = np.random.rand(1000,20)\n","# 모델이 학습 후 실제로 성능을 평가하는 입력 데이터.\n","# x_train 과 같은 feature 의 값을 가져야하고 처음보는 데이터로 평가해야하므로 새로운 데이터를 가져옴.\n","Y_test = np.random.randint(2, size=(1000,1))\n","# x_test 에 대한 결과값 (label)\n","\n","loss,accuracy = model.evaluate(x_test,Y_test)\n","print(f\"loss :{loss}\")\n","print(f\"accuracy :{accuracy}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k0vtFD3abhcw","executionInfo":{"status":"ok","timestamp":1742203820777,"user_tz":0,"elapsed":3848,"user":{"displayName":"uuny E","userId":"11907513080309152518"}},"outputId":"7963f29a-6111-4488-a4c3-bfbddc30b2e5"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.4307 - loss: 0.7045\n","Epoch 2/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5127 - loss: 0.6950  \n","Epoch 3/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5077 - loss: 0.6961 \n","Epoch 4/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5097 - loss: 0.6883 \n","Epoch 5/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5419 - loss: 0.6867 \n","Epoch 6/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5657 - loss: 0.6837 \n","Epoch 7/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6139 - loss: 0.6775 \n","Epoch 8/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6030 - loss: 0.6707 \n","Epoch 9/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5491 - loss: 0.6786 \n","Epoch 10/10\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5931 - loss: 0.6759  \n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5056 - loss: 0.6972\n","loss :0.6953048706054688\n","accuracy :0.515999972820282\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"2Z5SxvbecIsP"},"execution_count":null,"outputs":[]}]}